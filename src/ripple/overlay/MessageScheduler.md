# MessageScheduler

At times, our server needs to communicate with its peers:
sending them transactions, proposals, and validations, or requesting data.
We need a component to facilitate this communication that balances concerns
including throughput, congestion, and prioritization.

## Motivation

The existing architecture for communication is designed for this pattern:

- Each sender builds a fixed size `PeerSet` holding a subset of connected
    peers and sends messages to those peers whenever they want.
- Each receiver is a singleton and all responses of a given message type are
    delivered to the same receiver.

In the existing architecture,
if we want to add a new component that sends an existing message type
and thus receives an existing message type,
then we must find some way to divert responses to our messages
away from the existing singleton receiver.
Typically, that means attaching an identifier to the request
that is copied to the response,
and reserving a subset of those identifiers for our component,
so that when they are spotted in a response,
the dispatcher knows to forward them to our component.
Only two existing response types have this feature:
`TMLedgerData` and `TMGetObjectByHash`.
For these responses, and others we may add with this feature,
we want to relieve senders from the responsibility of managing request IDs.
We want to let them just send a messsage with an arbitrary receiver callback.

In the existing architecture,
a sender is not notified when peers in its `PeerSet` disconnect,
becoming unavailable to receive messages
or respond to any outstanding requests.
It is up to the sender to detect this condition,
add new connected peers to its set,
and timeout oustanding requests.
We want to relieve senders from these responsibilities.

In the existing architecture,
`PeerSet` starts with small subset of connected peers, typically two,
and does not scale up until some condition (chosen and enforced by the sender)
is met, typically a late or missing response.
We want to let senders who generate high volumes of traffic
(e.g. for downloading full ledgers)
immediately use as many connected peers as are available.

With great power comes great responsibility, however.
At different times we may be sending more messages than we are receiving,
or vice versa.
We need to strike a balance between our ability to flood peers with
messages and their ability to read and respond to those messages.
If not, then we run the risk of overwhelming our peers in traffic
indistinguishable from a denial-of-service attack.

The existing architecture leaves it up to each sender to moderate its traffic,
but senders are completely ignorant of each other,
so they cannot moderate the traffic of the application as whole.
To cap the volume of traffic generated by the application requires the ability
to make some senders wait while others take their turn,
and to arrange those turns "fairly" requires priority-based scheduling
that can only be implemented by a central message broker.

Most of the time, senders don't care which specific peer they send
a particular message to,
but the existing architecture encourages a sender to lock itself to a subset
of connected peers for the duration of its communication.
We want to let indiscriminate senders send their next message to any available
peer, selected by the broker according to its best judgment,
based on factors like latency and load balancing.


## Related Work

Each message to a peer effectively corresponds to a remote job executed on that
peer.
We already recognize that local threads are a limited resource,
and that local jobs are a demand on that resource.
We throttle the number of simultaneously executing local jobs to avoid
overloading the underlying machine's capacity for simultaneously executing
threads,
and we schedule local jobs according to priority during periods of high
contention.
We should do the same for remote jobs, i.e. messages.

The model for a message scheduler can be informed by our model of a job queue.
With the job queue, callers may submit as many jobs as they want.
Each job is a closure representing a unit of work,
and scheduling a job means requesting an idle thread to perform that work.
Jobs that cannot be immediately executed are queued.
Data for a job can continue to accumulate outside of the closure in
anticipation of the job's execution.
A job is finished when it returns,
and after it finishes, its thread is returned to the pool to be
allocated for another job.

Our message scheduler has some similarities.
Senders should be able to submit as many remote jobs as they want.
Each remote job is a closure representing a unit of work,
and scheduling a remote job means requesting an idle peer to perform that
work.
Remote jobs that cannot be immediately executed should be queued.
Data for a remote job should continue to accumulate outside of the closure in
anticipation of the job's execution.
A remote job is finished when it returns a response,
and after it finishes, its peer is returned to the pool to be allocated for
another remote job.

A peer scheduler has some differences too.
Unlike a job, a sender starting a remote job (by sending a request message)
cannot guarantee that it will terminate (by receiving a response message).
Thus we need timeouts to guarantee bounded termination.

Unlike local jobs where threads are isomorphic,
peers are distinguishable.
When a sender first sends a request,
it may not care which peer is chosen to receive it,
but when it retries a request, it likely will,
perhaps preferring to send it to a different peer in hopes that it will be
more likely to respond with the desired data in a reasonable time.


## Definitions

Before diving into the design discussion,
I want to introduce terminology to frame that discussion.

- **peer**: A connected peer server. Each peer has one or more channels.
    Peers do not necessarily have the same numbers of channels.
- **channel**: The unit of capacity for messaging a peer.
    A channel is either **open** (ready to send exactly one message to the
    peer) or
    **closed** (giving the peer time to read and respond to the last
    message sent).
- **message**: A message sent to or received from a peer. A message is either
    a request, a response, or a notification.
- **request**: A message sent to a peer that expects a response.
- **response** A message received from a peer in response to a request.
- **notification**: A message sent to a peer that expects no response.
- **sender**: An object that is waiting for an open channel to send a message.
- **receiver**: An object that is waiting for a response.


## Interface

Let us walk through the typical workflow of a request from the perspective of
a dependent.

The first step is to schedule a **sender**, adding it to a queue of senders
ordered by priority.[^5]
A sender represents a demand for at least one open channel to which to send
messages.
Each channel represents the capacity to send one message to one peer.
The sender has a callback method, `onOffer`, that accepts an **offer**.
The `onOffer` callback may be called immediately,
in the same thread that scheduled the sender.
If the caller wants the offer to come in a different thread,
perhaps to avoid blocking the current thread or double locking a mutex,
then it should schedule a job that will schedule the sender in another thread.

[^5]: For now, the priority order is just first come, first served.
We want to consider more sophisticated priority schemes for the future.

Eventually an offer will be made by passing it to `sender.onOffer`.
An offer contains N open channels,
and permits the sender to send messages to a subset M &lt;= N of them.
M is called the **size** of the offer.
The sender can iterate through the open channels in the offer,
selecting the ones it likes,
sending messages to as many or as few as it wants,
until (a) it has no more messages to send,
(b) the offer is exhausted, or
(c) it has considered every channel in the offer.
Sending a message to a channel closes the channel.
Once a channel is closed, it may not be used to send another message.

If a sender expects a response to its message, then we call that message
a **request**.
If it does not expect a response to its message, then we call that message
a **notification**.
Whenever it sends a request,
the sender must pass to the scheduler a **receiver**
that will wait for and react to the response.

The sender must set a timeout for each message it sends.
For requests, this timeout reflects the time after which a response is
considered missing and the sender may retry.
For notifications, this timeout reflects the time it expects the peer to need
to digest the notification,
effectively limiting the message rate of senders.

If the sender closed any channels, it will be removed from the sender queue.
If it closed no channels, perhaps because it is waiting for a specific peer,
then it will be left in the queue to see another, different offer later.
If it closed a channel, but wants more, perhaps because it has more messages
to send than there were open channels in the offer,
then it must schedule another sender.

When the response arrives, the receiver's success callback will be called with
it.
If, before that happens, the timeout expires or the peer disconnects,
then the receiver's failure callback will be called with an error code.
Either way, it may choose to schedule another sender, looping back around.


## Design

### Offers

An offer is a negotiation between the scheduler and a sender:
the scheduler is _offering_ some subset of open channels.
The full set of open channels is included in the offer to give the sender the
chance to select its favorite subset based on any arbitrary condition.
But if multiple senders are waiting, then we don't want to offer the use of
_every_ open channels to a sender, or else a single high priority sender can hog
every open channel indefinitely, starving other senders.
We want a "fair" distribution of open channels among senders that respects
priority but avoids starvation.
The ability to offer M of N peers gives us flexibility to implement different
fairness strategies.
For now, the strategy is for M to be 1 as long as multiple senders are waiting,
and N otherwise.

If a sender closes channels from an offer but is not wholly satisfied,
then it may schedule another "continuing" sender to wait for
another offer.
That continuing sender will be placed in the queue by priority (currently
ordered by arrival), and if there are any remaining open channels after offering
them to any higher priority senders, then they will be offered to the
continuing sender.

When a sender is offered open channels, it may filter through them and close
the ones it prefers, up to the offer's size.
If it does not close any channels, then it will remain in its place in the
queue and wait for another offer containing a different set of open channels.
That set is guaranteed to be different, but it may overlap, even significantly.
In fact, it is likely that the next offer will be simply one new open channel
plus the set in the previous offer.


### Senders and Receivers

When scheduling a sender, we pass a sender.
When sending a message, we pass a receiver.
In the implementation, these objects are represented by
raw pointers to abstract base classes.

The sender has two callback methods: one for sending messages to open channels,
and one for discarding the sender (called only when the scheduler is stopping).
The receiver has two callback methods: one for success (which means receiving
a response) and one for failure (which can mean a number of reasons).

The scheduler holds senders and receivers by a pointer that the scheduler does
not own and will not delete so that static objects can be used as senders and
receivers if desired.
Receiver callbacks are called with the request ID (returned by the method that
sends a request) so that the same receiver can be used for different requests.


### Lifetimes

Even though the message scheduler takes senders and receivers by non-owning
pointers,
to permit the use of long-living senders and receivers,
some of them may be short-living and require deletion.

Senders and receivers are guaranteed that either
(a) exactly one of their callbacks will be called exactly once,
in which case they can delete themselves, or
(b) they will be returned to a **withdrawer**,
who is then responsible for deleting them.
We choose the name "withdraw" over "cancel" to convey that the sender or
receiver will be returned, not just discarded.[^7]

[^7]: Withdrawal is not yet implemented.

The message scheduler should be constructed as a lower layer,
before anything that depends on it.
That means dependents should be destroyed before the scheduler is destroyed,
but that may leave waiting senders or receivers with dangling references to
those dependents.
There are two ways to handle this situation:

1. Let callers withdraw both senders and receivers.
2. Let a caller "stop" the scheduler from calling any more senders or receivers.

When stopping, the scheduler immediately fails all waiting receivers
(with the reason "shutdown")
and discards all waiting senders,
and never again accepts a sender or receiver.


### Peers

The lifetimes of peer objects are not managed by the peer scheduler.
They are managed by the overlay and the peer objects themselves.[^3]
The scheduler holds only `std::weak_ptr`s to the peer objects in its pool.
A peer offer is a set of `std::weak_ptr`s.
As a sender iterates through an offer, those `std::weak_ptr`s are locked to
`std::shared_ptr`s.
If it cannot be locked, then the peer object has already been destroyed, and it
will be skipped over, silently.
(This means it is possible that a sender receives an offer that has
a smaller true size than what is suggested, and even possibly zero size.)
As an offer discovers it is holding dead peers, it removes those peers.
An offer that is a priori known to be empty is never passed to a sender.

[^3]: When a `PeerImp` is constructed, it starts a timer.
At first, that timer callback holds the only `std::shared_ptr` owning the `PeerImp`,
and the `OverlayImpl` holds a table of `std::weak_ptr<PeerImp>`s.
Copies of the `std::shared_ptr` are made only when callers are ready to send
a message.
Whenever the `PeerImp` hears a heartbeat from the connected peer, it resets the timer.
If the timer expires before the next heartbeat arrives, then the
`std::shared_ptr` in the callback is destroyed, and as soon as no other scope
is holding a copy of the same `std::shared_ptr`, then the `PeerImp` is destroyed.


### Locks

The message scheduler maintains two locks:
one for "offers", i.e. the queue of senders and the pool of open channels,
together;
and one for "requests", i.e. a table of waiting receivers.

All callbacks are called while one of the locks is held.
This way, they can benefit from the assumption that they are not executing
concurrently.[^8]
Callbacks should be very short then.
A callback that wants to do significant work should just schedule a job for that
work and return.

[^8]: Maybe we don't need this?

Only `Sender::onOffer` should call `MessageScheduler::send` (via
`PeerOffer::Iterator::send`), which locks the requests.
`Sender::onOffer` is called while the offers are locked,
which means that we must be careful to always lock the offers before the
requests in scenarios where we need to lock both, in order to avoid a deadlock.

`MessageScheduler::schedule` may be called from any sender or receiver callback.
It typically locks the offers, but this is not safe in two circumstances:

- When a receiver callback is called, the requests are locked.
- When `Sender::onOffer` is called, the offers are already locked.

For these reasons, `MessageScheduler::schedule` uses a thread-local variable
to detect when it is called in a thread that already owns either lock.
